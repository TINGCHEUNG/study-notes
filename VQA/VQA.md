# Visual Question Answering(VQA) Summary

## Papers

### Surveys

> Survey of Visual Question Answering: Datasets and Techniques 
[paper](https://arxiv.org/pdf/1705.03865.pdf)

> Information fusion in visual question answering: A Survey (2019)
[paper](https://pdf.sciencedirectassets.com/272144/1-s2.0-S1566253518X00098/1-s2.0-S1566253518308893/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEB8aCXVzLWVhc3QtMSJIMEYCIQDa%2BZnt5QuIZWvmzJC8rgmrXpWzJ4FgA3gHKo6ZsxGZBAIhAOpJiJaT0KZEq9hH5nPFVj0HEHTpxVI3LLAOKS7jXTaEKr0DCIj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAhoMMDU5MDAzNTQ2ODY1IgxgnuHoexKKF%2Bgw6RwqkQNplaFErUt8z2qsKCimnoehuhSwAx77mktm%2BLS9twP5gCnoId6PxzbZnQa1k6JTPa2GQ6TE80FuDAuAwahtk%2FBD6MFjihOAVWZLUmrPZ3fio7qp8G8rAnelVBAEAmzIMJpfHzjAYAUfdZYXElDfwhBeJ9Um96TkRO4JCNf2tl3tX0ICtBkZLvHjwaj4S%2FDcpUimCkZ7%2FYLGpimim2J9kZMDq4qLnEcPj0oA7Bq15vz8BLR27e38%2Bf2SgNrl7TqsydtVzm96k6v5nWl86IjNQeZ8dzAz3yhnJYkZj35nSVa%2FBCJD%2FvPpahew1GEf%2BEujsuzGTomrooZ97%2FEkZoOhAB704JzxLL3g%2FP33Fh8pMEyYaBpm1vuRQGqNVKKb8rWO74EXA4IubKpmuQlYGbWTGrsBuX5oEwe2BgBA8VN6LS58odXfIziF%2Be7EQF6HmvLxS2kSA%2B4zasQ6nfDmbBBaQcsouH%2F6PSWkfl6vsEv6mzX98WwxiDOf0co7Mp7nzQUgNPl10dpF70ndOMfvL11%2FdkVJwjDUzJbwBTrqAZ6YTjSguUOlMrI%2BJlogU%2BvnCNNTAnM82MJx4QEcdPUrYT2ZgJnFTl%2FNWWEA4%2FqSXIB8ixKvlr3wp9RO%2FAFI2FHOW9X4aMvMRov5Jf9WozXzmb4vrQvac%2F7WrL4S3XFd2mjQu4VUwg31%2FVVkzhqcaH4Lq5skbf3XeoTrf6NF2%2FJqB5LCDSrcmp%2FAg1pjVO5MXslDuO5pZR0ogf9I0OmmXwX47pxJQ7YEwb%2FJN%2FPPHS2v3XeG0h8dzSbqMpno1oXS69kdibQG8GsqbMOJ4Tcv0WLY5G0yYD04aTPbMeI2IeIJrEA5w%2BcxwFhWAA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20191227T073459Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYXIR7436B%2F20191227%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=7545b0fb47586d4b31e3836bf66daecba73effa5bb4321691dc2e883ea60e249&hash=6856ca38fcc0cea24e46601c8d5408436e2f0fddabeccb2c7ceea96e608abbe9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1566253518308893&tid=spdf-3fd1eb27-5bb1-47a1-a40f-40c642ab8be0&sid=d3cc9df811b39643ed3b40e78d72ab5309a7gxrqa&type=client)

> Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge (CVPR2018)
[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf)

> Visual Question Answering: Datasets, Methods, Challenges and Oppurtunities (2018)
[paper](https://www.cs.princeton.edu/courses/archive/spring18/cos598B/public/projects/LiteratureReview/COS598B_spr2018_VQAreview.pdf)

### Based on Knowledge

> Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries 
[paper](https://arxiv.org/abs/1507.05670)

> Ask Me Anything: Free-form Visual Question AnsweringBased on Knowledge from External Sources (cvpr2016) 
[paper](https://arxiv.org/abs/1511.06973)

> FVQA: Fact-based Visual Question Answering 
[paper](https://arxiv.org/abs/1606.05433)

> Explicit Knowledge-based Reasoning for Visual Question Answering (IJCAI2017)
[paper](https://www.ijcai.org/proceedings/2017/179)
[blog](https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80851407)

> Knowledge Acquisition for Visual Question Answering via Iterative Querying (cvpr2017)
[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Knowledge_Acquisition_for_CVPR_2017_paper.pdf)

> R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering (KDD2018)
[paper](https://arxiv.org/abs/1805.09701)
[code](https://github.com/lupantech/rvqa)

> Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding (NeurIPS 2018)
[paper](https://arxiv.org/abs/1810.02338)
[code](https://github.com/kexinyi/ns-vqa)
[blog1](https://zhuanlan.zhihu.com/p/61533835)
[blog2](https://zhuanlan.zhihu.com/p/46392910)

> Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering (NIPS2018)
[paper](https://arxiv.org/abs/1811.00538)

> Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks (KDD2019)
[paper](https://arxiv.org/abs/1905.08865)

> From Strings to Things: Knowledge-enabled VQA Models that can Read and Reason (ICCV2019)
[paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_From_Strings_to_Things_Knowledge-Enabled_VQA_Model_That_Can_Read_ICCV_2019_paper.pdf)

> Learning to Compose Dynamic Tree Structures for Visual Contexts (cvpr2019)
[paper](https://zpascal.net/cvpr2019/Tang_Learning_to_Compose_Dynamic_Tree_Structures_for_Visual_Contexts_CVPR_2019_paper.pdf)

> KVQA: Knowledge-aware Visual Question Answering(AAAI2019)
[paper](https://www.aaai.org/ojs/index.php/AAAI/article/view/4915)
[code](http://malllabiisc.github.io/resources/kvqa/)

> OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge (cvpr2019)
[paper](https://arxiv.org/abs/1906.00067)

> MUREL: Multimodal Relational Reasoning for Visual Question Answering (cvpr2019)
[paper](https://arxiv.org/abs/1902.09487)
[code](https://github.com/Cadene/murel.bootstrap.pytorch)
[blog](https://zhuanlan.zhihu.com/p/94553059)

> WK-VQA(need feedback in GitHub)
[paper](https://github.com/sanket0211/WK-VQA/)


### Based on GNN

> Graph Reasoning Networks for Visual Question Answering (Arxiv)
[paper](https://arxiv.org/pdf/1907.09815.pdf)

> Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering (NIPS2018)
[paper](https://arxiv.org/abs/1811.00538)

> Relation-aware Graph Attention Network for Visual Question Answering (ICCV2019)
[paper](https://arxiv.org/pdf/1903.12314.pdf)
[blog](https://zhuanlan.zhihu.com/p/63820622)
[code](https://github.com/linjieli222/VQA_ReGAT)

### Based on ML

> Learning Visual Question Answering byBootstrapping Hard Attention
[paper](https://arxiv.org/pdf/1808.00300.pdf)
[blog](https://zhuanlan.zhihu.com/p/41546921)

> BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection
[paper](https://arxiv.org/abs/1902.00038)
[code](https://github.com/Cadene/block.bootstrap.pytorch)

> awesome-vqa [link](https://github.com/chingyaoc/awesome-vqa)

### Relational Reasoning

> A simple neural network module for relational reasoning
[paper](https://arxiv.org/pdf/1706.01427.pdf)

## GitHub link about VQA

### VQA Summary(综述)

#### Summary

[视觉问答（VQA）综述](https://github.com/seagle0128/Algorithm_Interview_Notes-Chinese/blob/master/_papers/QA-%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94-A-%E7%BB%BC%E8%BF%B0.md)  
[Knowledge-Based Visual Reasoning (dynamic updating)](https://github.com/Sympathize/vkr-papers)  
[Awesome_VQA](https://github.com/waallf/Awesome_VQA)

#### codes

https://github.com/liuzhi136/Visual-Question-Answering  
https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering  
https://github.com/paarthneekhara/neural-vqa-tensorflow  
https://github.com/Cadene/block.bootstrap.pytorch  
https://github.com/Cadene/vqa.pytorch  
https://github.com/KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch  
https://github.com/peteanderson80/bottom-up-attention  
https://github.com/paarthneekhara/neural-vqa-tensorflow

## Other links about VQA

> 最新7篇视觉问答（VQA）相关论文—解释、读写记忆网络、逆视觉问答、视觉推理、可解释性、注意力机制、计数 [link](https://cloud.tencent.com/developer/article/1086325)

> 一文带你了解深度学习中的视觉问答（VQA）技术 [link](https://zhuanlan.zhihu.com/p/34312290)

> 「自然语言处理(NLP)」---亚马逊QA(含源码) && 视觉问答QA [link](https://zhuanlan.zhihu.com/p/78357484)

> 【CV+NLP】更有智慧的眼睛：图像描述（Image Caption）&视觉问答（VQA）综述 [link](https://zhuanlan.zhihu.com/p/52499758)

> 一文看懂深度学习中的VQA(视觉问答)技术 [link1](https://zhuanlan.zhihu.com/p/35305264) [link2](https://www.jianshu.com/p/76d2e081e303)

> 基于深度学习的VQA（视觉问答）技术 [link](https://zhuanlan.zhihu.com/p/22530291)

## Application about VQA

1. image retrieval
2. aided-navigation for blind individuals
3. automatic querying of surveillance video
